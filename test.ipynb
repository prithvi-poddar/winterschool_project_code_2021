{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "from torch.jit import Error\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.parallel\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TNet(nn.Module):\n",
    "    def __init__(self, n_feats, n_samples):\n",
    "        super(TNet, self).__init__()\n",
    "        self.n_feats = n_feats\n",
    "        self.conv1 = nn.Conv1d(n_feats, 64, 1)\n",
    "        self.conv2 = nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = nn.Conv1d(128, 1024, 1)\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, n_feats*n_feats)\n",
    "        self.bn1 = nn.LayerNorm(T.Size([64, n_samples]))\n",
    "        self.bn2 = nn.LayerNorm(T.Size([128, n_samples]))\n",
    "        self.bn3 = nn.LayerNorm(T.Size([1024,n_samples]))\n",
    "        self.bn4 = nn.LayerNorm(512)\n",
    "        self.bn5 = nn.LayerNorm(256)\n",
    "        \n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size()[0]\n",
    "        bias = Variable(torch.from_numpy(np.eye(self.n_feats).flatten().astype(np.float32))).view(1,self.n_feats*self.n_feats).repeat(batch_size,1).to(self.device)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "\n",
    "        x = nn.MaxPool1d(x.size(-1))(x)\n",
    "        x = nn.Flatten(1)(x)\n",
    "\n",
    "        x = F.relu(self.bn4(self.fc1(x)))\n",
    "        x = F.relu(self.bn5(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        x = x + bias\n",
    "        x = x.view(-1, self.n_feats, self.n_feats)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PointNet(nn.Module):\n",
    "    def __init__(self, mode='classifier', n_samples=1024, n_classes=10):\n",
    "        super(PointNet, self).__init__()\n",
    "        self.in_trans = TNet(3, n_samples)\n",
    "        self.feat_trans = TNet(64, n_samples)\n",
    "        self.mode = mode\n",
    "        self.n_samples = n_samples\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        if(self.mode!='classifier' and self.mode!='segmentation'):\n",
    "            raise Exception(\"wrong mode selected! choose 'classifier' or 'segmentation'\")\n",
    "\n",
    "        self.mlp1 = nn.Sequential(nn.Conv1d(3, 64, 1),\n",
    "                                nn.LayerNorm(T.Size([64, n_samples])),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Conv1d(64, 64, 1),\n",
    "                                nn.LayerNorm(T.Size([64, n_samples])),\n",
    "                                nn.ReLU())\n",
    "\n",
    "        self.mlp2 = nn.Sequential(nn.Conv1d(64, 64, 1),\n",
    "                                nn.LayerNorm(T.Size([64, n_samples])),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Conv1d(64, 128, 1),\n",
    "                                nn.LayerNorm(T.Size([128, n_samples])),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Conv1d(128, 1024, 1),\n",
    "                                nn.LayerNorm(T.Size([1024, n_samples])),\n",
    "                                nn.ReLU())\n",
    "\n",
    "        self.mlp3 = nn.Sequential(nn.Linear(1024, 512),\n",
    "                                    nn.LayerNorm(512),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(512, 256),\n",
    "                                    nn.LayerNorm(256),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Dropout(0.3),\n",
    "                                    nn.Linear(256, n_classes),\n",
    "                                    nn.Softmax(1))\n",
    "\n",
    "        self.mlp4 = nn.Sequential(nn.Conv1d(1088, 512, 1),\n",
    "                                nn.LayerNorm(T.Size([512, n_samples])),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Conv1d(512, 256, 1),\n",
    "                                nn.LayerNorm(T.Size([256, n_samples])),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Conv1d(256, 128, 1),\n",
    "                                nn.LayerNorm(T.Size([128, n_samples])),\n",
    "                                nn.ReLU())\n",
    "\n",
    "        self.mlp5 = nn.Sequential(nn.Conv1d(128, n_classes, 1),\n",
    "                                nn.LayerNorm(T.Size([n_classes, n_samples])),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Softmax(1))\n",
    "\n",
    "        # self.optimizer = optim.Adam(self.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.device)\n",
    "\n",
    "        input_transform = self.in_trans.forward(T.transpose(x,1,2))\n",
    "        x = T.bmm(x, input_transform) # nx3\n",
    "\n",
    "        x = T.transpose(self.mlp1(T.transpose(x,1,2)), 1, 2) # nx64\n",
    "        feat_transform = self.feat_trans(T.transpose(x,1,2))\n",
    "        x_to_concat = T.bmm(x, feat_transform) # nx64\n",
    "\n",
    "        x = T.transpose(self.mlp2(T.transpose(x_to_concat,1,2)), 1, 2) # nx1024\n",
    "        global_feat = T.max(x, 1, keepdim=True)[0].view(-1, 1024)\n",
    "\n",
    "        if self.mode == 'classifier':\n",
    "            x = self.mlp3(global_feat)\n",
    "            return x, feat_transform\n",
    "\n",
    "        elif self.mode == 'segmentation':\n",
    "            x = global_feat.view(-1, 1, 1024).repeat(1, self.n_samples, 1)\n",
    "            x = T.cat([x_to_concat, x], 2)\n",
    "            point_feats = T.transpose(self.mlp4(T.transpose(x,1,2)), 1, 2)\n",
    "            x = T.transpose(self.mlp5(T.transpose(point_feats,1,2)), 1, 2)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0593, 0.0593, 0.0593, 0.2791, 0.0593, 0.0820, 0.0593, 0.0593, 0.0941,\n",
       "         0.1888],\n",
       "        [0.0522, 0.0522, 0.0522, 0.2956, 0.0674, 0.0835, 0.0522, 0.0522, 0.1251,\n",
       "         0.1674],\n",
       "        [0.0515, 0.0515, 0.0515, 0.2600, 0.0747, 0.0947, 0.0515, 0.0567, 0.1653,\n",
       "         0.1424],\n",
       "        [0.0579, 0.0579, 0.0579, 0.2371, 0.0662, 0.0958, 0.0579, 0.0579, 0.1278,\n",
       "         0.1837],\n",
       "        [0.0607, 0.0607, 0.0607, 0.2247, 0.0658, 0.0917, 0.0607, 0.0607, 0.0996,\n",
       "         0.2145],\n",
       "        [0.0438, 0.0438, 0.0438, 0.3010, 0.0785, 0.1091, 0.0438, 0.0438, 0.1870,\n",
       "         0.1054],\n",
       "        [0.0564, 0.0564, 0.0564, 0.2599, 0.0672, 0.0861, 0.0564, 0.0748, 0.1259,\n",
       "         0.1605],\n",
       "        [0.0504, 0.0504, 0.0504, 0.2669, 0.0718, 0.0958, 0.0504, 0.0504, 0.1672,\n",
       "         0.1463],\n",
       "        [0.0606, 0.0606, 0.0606, 0.2353, 0.0687, 0.0782, 0.0606, 0.0606, 0.0870,\n",
       "         0.2279],\n",
       "        [0.0594, 0.0594, 0.0594, 0.2435, 0.0656, 0.0773, 0.0594, 0.0594, 0.0860,\n",
       "         0.2305]], device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = PointNet(mode='segmentation', n_samples=10, n_classes=10).to(T.device('cuda:0'))\n",
    "x = T.rand(32,10,3).to(T.device('cuda:0'))\n",
    "x = net(x)\n",
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1024])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = T.rand(1,10, 1024)\n",
    "a = torch.max(a, 1, keepdim=True)[0]\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0686, 0.0737, 0.0573, 0.0617, 0.0930, 0.0412, 0.0974, 0.0720, 0.0720,\n",
       "         0.0791],\n",
       "        [0.0915, 0.0724, 0.0474, 0.0399, 0.0515, 0.0858, 0.0629, 0.0499, 0.0626,\n",
       "         0.0934],\n",
       "        [0.0848, 0.0594, 0.0431, 0.1072, 0.0885, 0.0718, 0.0462, 0.0586, 0.0489,\n",
       "         0.0780],\n",
       "        [0.0682, 0.0459, 0.0596, 0.0494, 0.1031, 0.0664, 0.0581, 0.0925, 0.0912,\n",
       "         0.0740],\n",
       "        [0.0630, 0.0606, 0.0806, 0.0496, 0.0394, 0.0604, 0.0487, 0.0692, 0.0647,\n",
       "         0.0910],\n",
       "        [0.0342, 0.0470, 0.0575, 0.0838, 0.0730, 0.0868, 0.0561, 0.0746, 0.0478,\n",
       "         0.0664],\n",
       "        [0.0460, 0.0758, 0.1036, 0.0470, 0.0405, 0.0753, 0.0965, 0.0894, 0.0470,\n",
       "         0.0781],\n",
       "        [0.0861, 0.0954, 0.0505, 0.0867, 0.0433, 0.0620, 0.0705, 0.0596, 0.0936,\n",
       "         0.0499],\n",
       "        [0.0419, 0.0619, 0.0882, 0.0875, 0.1023, 0.0461, 0.0682, 0.0408, 0.0812,\n",
       "         0.0550],\n",
       "        [0.0520, 0.0925, 0.0869, 0.0908, 0.0403, 0.0619, 0.0474, 0.0718, 0.0832,\n",
       "         0.0404],\n",
       "        [0.0604, 0.0364, 0.0538, 0.0586, 0.0608, 0.0404, 0.0569, 0.0413, 0.0891,\n",
       "         0.0640],\n",
       "        [0.0449, 0.0806, 0.0800, 0.0625, 0.0667, 0.0631, 0.0677, 0.0787, 0.0418,\n",
       "         0.0463],\n",
       "        [0.0903, 0.0638, 0.0519, 0.0417, 0.0780, 0.0713, 0.0962, 0.0361, 0.0503,\n",
       "         0.0637],\n",
       "        [0.0913, 0.0550, 0.0575, 0.0617, 0.0705, 0.0620, 0.0677, 0.0830, 0.0496,\n",
       "         0.0544],\n",
       "        [0.0767, 0.0799, 0.0821, 0.0719, 0.0493, 0.1055, 0.0597, 0.0825, 0.0770,\n",
       "         0.0663]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = T.rand(32, 10, 15)\n",
    "x = T.transpose(nn.Softmax(1)(x), 1, 2)\n",
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9998"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.array([0.0593, 0.0593, 0.0593, 0.2791, 0.0593, 0.0820, 0.0593, 0.0593, 0.0941,\n",
    "         0.1888]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6382069a7793f11ca2e4d453d6ea9dbfa636d21dc26e68dacc4c30284fcf592e"
  },
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('prithvi': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}